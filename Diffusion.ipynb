{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "D0wiphC9Tzjj"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odjb7JY1nGSj"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "References:\n",
        "https://medium.com/@vedantjumle/image-generation-with-diffusion-models-using-keras-and-tensorflow-9f60aae72ac\n",
        "https://arxiv.org/abs/2006.11239\n",
        "https://arxiv.org/abs/2010.02502\n",
        "'''\n",
        "\n",
        "!pip install tensorflow\n",
        "!pip install tensorflow_datasets\n",
        "!pip install tensorflow_addons\n",
        "!pip install tensorflow_federated\n",
        "!pip install einops\n",
        "!rm -r sample_data/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import inspect\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.layers as nn\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_federated as tff\n",
        "from tensorflow import keras, einsum\n",
        "from tensorflow.keras import Model, Sequential\n",
        "from tensorflow.keras.layers import Layer\n",
        "from einops import rearrange\n",
        "from einops.layers.tensorflow import Rearrange\n",
        "from functools import partial\n",
        "\n",
        "batches   = 64\n",
        "timesteps = 200\n",
        "epochs    = 1\n",
        "\n",
        "##### DATASET MNIST #####\n",
        "def preprocess(x, y):\n",
        "    return tf.image.resize(tf.cast(x, tf.float32) / 127.5 - 1, (32, 32))\n",
        "def get_datasets():\n",
        "    train_ds = tfds.load('mnist', as_supervised=True, split='train')\n",
        "    train_ds = train_ds.map(preprocess, tf.data.AUTOTUNE)\n",
        "    train_ds = train_ds.shuffle(5000).batch(batches).prefetch(tf.data.AUTOTUNE)\n",
        "    return tfds.as_numpy(train_ds)\n",
        "dataset = get_datasets() # shape = (60000, 32, 32, 1)\n",
        "##### DATASET MNIST #####\n",
        "\n",
        "X = np.random.randint(low=1, high=5, size=(1000, 16, 16, 16))\n",
        "X = tf.data.Dataset.from_tensor_slices(X)\n",
        "X = X.shuffle(1).batch(batches)\n",
        "X = tfds.as_numpy(X)\n",
        "#dataset = X\n",
        "example_shape = next(x.shape for x in iter(dataset))\n",
        "shape = example_shape[1:3]\n",
        "channels = example_shape[3]\n",
        "\n",
        "def set_key(key):\n",
        "    ''' Random seed control '''\n",
        "    np.random.seed(key)\n",
        "\n",
        "def forward_noise(key, x_0, t):\n",
        "    ''' Forward noise function '''\n",
        "    set_key(key)\n",
        "    b = np.linspace(0.0001, 0.02, timesteps)\n",
        "    a = 1 - b\n",
        "    a_ = np.cumprod(a, 0)\n",
        "    a_ = np.concatenate((np.array([1.]), a_[:-1]), axis=0)\n",
        "    sqrt_a_ = np.sqrt(a_)\n",
        "    sqrt_1_a_ = np.sqrt(1 - a_)\n",
        "    noise = np.random.normal(size=x_0.shape)\n",
        "    reshaped_sqrt_a_t = np.reshape(np.take(sqrt_a_, t), (-1, 1, 1, 1))\n",
        "    reshaped_sqrt_1_a_t = np.reshape(np.take(sqrt_1_a_, t), (-1, 1, 1, 1))\n",
        "    noisy_data = reshaped_sqrt_a_t * x_0 + reshaped_sqrt_1_a_t * noise\n",
        "    return noisy_data, noise\n",
        "\n",
        "def generate_timestamp(key, num):\n",
        "    ''' Generate the timesteps '''\n",
        "    set_key(key)\n",
        "    return np.int32(np.random.uniform(size=[num], low=0, high=timesteps))\n",
        "\n",
        "def ddim(x_t, pred_noise, t, sigma_t):\n",
        "    ''' Backward denoise function using denoising diffusion implicit model '''\n",
        "    b = np.linspace(0.0001, 0.02, timesteps)\n",
        "    a = 1 - b\n",
        "    a_ = np.cumprod(a, 0)\n",
        "    a_ = np.concatenate((np.array([1.]), a_[:-1]), axis=0)\n",
        "    sqrt_a_ = np.sqrt(a_)\n",
        "    sqrt_1_a_ = np.sqrt(1 - a_)\n",
        "    a_t_bar = np.take(a_, t)\n",
        "    a_t_minus_one = np.take(a, t-1)\n",
        "    pred = (x_t - ((1 - a_t_bar) ** 0.5) * pred_noise)/ (a_t_bar ** 0.5)\n",
        "    pred = (a_t_minus_one ** 0.5) * pred\n",
        "    pred = pred + ((1 - a_t_minus_one - (sigma_t ** 2)) ** 0.5) * pred_noise\n",
        "    eps_t = np.random.normal(size=x_t.shape)\n",
        "    pred = pred + (sigma_t * eps_t)\n",
        "    return pred\n",
        "\n",
        "def loss_fn(real, generated):\n",
        "    ''' The mean squared error loss function '''\n",
        "    loss = tf.math.reduce_mean((real - generated) ** 2)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "bbkWY_IRmBWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# U-NET"
      ],
      "metadata": {
        "id": "D0wiphC9Tzjj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def exists(x):\n",
        "    return x is not None\n",
        "\n",
        "def default(val, d):\n",
        "    if exists(val):\n",
        "        return val\n",
        "    return d() if inspect.isfunction(d) else d\n",
        "\n",
        "class SinusoidalPosEmb(Layer):\n",
        "    def __init__(self, dim, max_positions=10000):\n",
        "        super(SinusoidalPosEmb, self).__init__()\n",
        "        self.dim = dim\n",
        "        self.max_positions = max_positions\n",
        "    def call(self, x, training=True):\n",
        "        x = tf.cast(x, tf.float32)\n",
        "        half_dim = self.dim // 2\n",
        "        emb = math.log(self.max_positions) / (half_dim - 1)\n",
        "        emb = tf.exp(tf.range(half_dim, dtype=tf.float32) * -emb)\n",
        "        emb = x[:, None] * emb[None, :]\n",
        "        emb = tf.concat([tf.sin(emb), tf.cos(emb)], axis=-1)\n",
        "        return emb\n",
        "        \n",
        "class Identity(Layer):\n",
        "    def __init__(self):\n",
        "        super(Identity, self).__init__()\n",
        "    def call(self, x, training=True):\n",
        "        return tf.identity(x)\n",
        "\n",
        "class Residual(Layer):\n",
        "    def __init__(self, fn):\n",
        "        super(Residual, self).__init__()\n",
        "        self.fn = fn\n",
        "    def call(self, x, training=True):\n",
        "        return self.fn(x, training=training) + x\n",
        "\n",
        "def Upsample(dim):\n",
        "    return nn.Conv2DTranspose(filters=dim, kernel_size=4, strides=2, padding='SAME')\n",
        "\n",
        "def Downsample(dim):\n",
        "    return nn.Conv2D(filters=dim, kernel_size=4, strides=2, padding='SAME')\n",
        "\n",
        "class LayerNorm(Layer):\n",
        "    def __init__(self, dim, eps=1e-5, **kwargs):\n",
        "        super(LayerNorm, self).__init__(**kwargs)\n",
        "        self.eps = eps\n",
        "        self.g = tf.Variable(tf.ones([1, 1, 1, dim]))\n",
        "        self.b = tf.Variable(tf.zeros([1, 1, 1, dim]))\n",
        "    def call(self, x, training=True):\n",
        "        var = tf.math.reduce_variance(x, axis=-1, keepdims=True)\n",
        "        mean = tf.reduce_mean(x, axis=-1, keepdims=True)\n",
        "        x = (x - mean) / tf.sqrt((var + self.eps)) * self.g + self.b\n",
        "        return x\n",
        "\n",
        "class PreNorm(Layer):\n",
        "    def __init__(self, dim, fn):\n",
        "        super(PreNorm, self).__init__()\n",
        "        self.fn = fn\n",
        "        self.norm = LayerNorm(dim)\n",
        "    def call(self, x, training=True):\n",
        "        x = self.norm(x)\n",
        "        return self.fn(x)\n",
        "\n",
        "class SiLU(Layer):\n",
        "    def __init__(self):\n",
        "        super(SiLU, self).__init__()\n",
        "    def call(self, x, training=True):\n",
        "        return x * tf.nn.sigmoid(x)\n",
        "\n",
        "def gelu(x, approximate=False):\n",
        "    if approximate:\n",
        "        coeff = tf.cast(0.044715, x.dtype)\n",
        "        return 0.5 * x * (1.0 + tf.tanh(0.7978845608028654 * (x + coeff * tf.pow(x, 3))))\n",
        "    else:\n",
        "        return 0.5 * x * (1.0 + tf.math.erf(x / tf.cast(1.4142135623730951, x.dtype)))\n",
        "\n",
        "class GELU(Layer):\n",
        "    def __init__(self, approximate=False):\n",
        "        super(GELU, self).__init__()\n",
        "        self.approximate = approximate\n",
        "    def call(self, x, training=True):\n",
        "        return gelu(x, self.approximate)\n",
        "\n",
        "class Block(Layer):\n",
        "    def __init__(self, dim, groups=8):\n",
        "        super(Block, self).__init__()\n",
        "        self.proj = nn.Conv2D(dim, kernel_size=3, strides=1, padding='SAME')\n",
        "        self.norm = tfa.layers.GroupNormalization(groups, epsilon=1e-05)\n",
        "        self.act = SiLU()\n",
        "    def call(self, x, gamma_beta=None, training=True):\n",
        "        x = self.proj(x)\n",
        "        x = self.norm(x, training=training)\n",
        "        if exists(gamma_beta):\n",
        "            gamma, beta = gamma_beta\n",
        "            x = x * (gamma + 1) + beta\n",
        "        x = self.act(x)\n",
        "        return x\n",
        "\n",
        "class ResnetBlock(Layer):\n",
        "    def __init__(self, dim, dim_out, time_emb_dim=None, groups=8):\n",
        "        super(ResnetBlock, self).__init__()\n",
        "        self.mlp = Sequential([\n",
        "            SiLU(),\n",
        "            nn.Dense(units=dim_out * 2)\n",
        "        ]) if exists(time_emb_dim) else None\n",
        "        self.block1 = Block(dim_out, groups=groups)\n",
        "        self.block2 = Block(dim_out, groups=groups)\n",
        "        self.res_conv = nn.Conv2D(\n",
        "            filters=dim_out, kernel_size=1, \n",
        "            strides=1) if dim != dim_out else Identity()\n",
        "    def call(self, x, time_emb=None, training=True):\n",
        "        gamma_beta = None\n",
        "        if exists(self.mlp) and exists(time_emb):\n",
        "            time_emb = self.mlp(time_emb)\n",
        "            time_emb = rearrange(time_emb, 'b c -> b 1 1 c')\n",
        "            gamma_beta = tf.split(time_emb, num_or_size_splits=2, axis=-1)\n",
        "        h = self.block1(x, gamma_beta=gamma_beta, training=training)\n",
        "        h = self.block2(h, training=training)\n",
        "        return h + self.res_conv(x)\n",
        "\n",
        "class LinearAttention(Layer):\n",
        "    def __init__(self, dim, heads=4, dim_head=32):\n",
        "        super(LinearAttention, self).__init__()\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.heads = heads\n",
        "        self.hidden_dim = dim_head * heads\n",
        "        self.attend = nn.Softmax()\n",
        "        self.to_qkv = nn.Conv2D(filters=self.hidden_dim * 3, kernel_size=1, strides=1, use_bias=False)\n",
        "        self.to_out = Sequential([\n",
        "            nn.Conv2D(filters=dim, kernel_size=1, strides=1),\n",
        "            LayerNorm(dim)])\n",
        "    def call(self, x, training=True):\n",
        "        b, h, w, c = x.shape\n",
        "        qkv = self.to_qkv(x)\n",
        "        qkv = tf.split(qkv, num_or_size_splits=3, axis=-1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b x y (h c) -> b h c (x y)', h=self.heads), qkv)\n",
        "        q = tf.nn.softmax(q, axis=-2)\n",
        "        k = tf.nn.softmax(k, axis=-1)\n",
        "        q = q * self.scale\n",
        "        context = einsum('b h d n, b h e n -> b h d e', k, v)\n",
        "        out = einsum('b h d e, b h d n -> b h e n', context, q)\n",
        "        out = rearrange(out, 'b h c (x y) -> b x y (h c)', h=self.heads, x=h, y=w)\n",
        "        out = self.to_out(out, training=training)\n",
        "        return out\n",
        "\n",
        "class Attention(Layer):\n",
        "    def __init__(self, dim, heads=4, dim_head=32):\n",
        "        super(Attention, self).__init__()\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.heads = heads\n",
        "        self.hidden_dim = dim_head * heads\n",
        "        self.to_qkv = nn.Conv2D(filters=self.hidden_dim * 3, kernel_size=1, strides=1, use_bias=False)\n",
        "        self.to_out = nn.Conv2D(filters=dim, kernel_size=1, strides=1)\n",
        "    def call(self, x, training=True):\n",
        "        b, h, w, c = x.shape\n",
        "        qkv = self.to_qkv(x)\n",
        "        qkv = tf.split(qkv, num_or_size_splits=3, axis=-1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b x y (h c) -> b h c (x y)', h=self.heads), qkv)\n",
        "        q = q * self.scale\n",
        "        sim = einsum('b h d i, b h d j -> b h i j', q, k)\n",
        "        sim_max = tf.stop_gradient(tf.expand_dims(tf.argmax(sim, axis=-1), axis=-1))\n",
        "        sim_max = tf.cast(sim_max, tf.float32)\n",
        "        sim = sim - sim_max\n",
        "        attn = tf.nn.softmax(sim, axis=-1)\n",
        "        out = einsum('b h i j, b h d j -> b h i d', attn, v)\n",
        "        out = rearrange(out, 'b h (x y) d -> b x y (h d)', x = h, y = w)\n",
        "        out = self.to_out(out, training=training)\n",
        "        return out\n",
        "\n",
        "class Unet(Model):\n",
        "    def __init__(self,\n",
        "                 dim=64,\n",
        "                 init_dim=None,\n",
        "                 out_dim=None,\n",
        "                 dim_mults=(1, 2, 4, 8),\n",
        "                 channels=3,\n",
        "                 resnet_block_groups=8,\n",
        "                 learned_variance=False,\n",
        "                 sinusoidal_cond_mlp=True):\n",
        "        super(Unet, self).__init__()\n",
        "        self.channels = channels\n",
        "        init_dim = default(init_dim, dim // 3 * 2)\n",
        "        self.init_conv = nn.Conv2D(\n",
        "            filters=init_dim, kernel_size=7, strides=1, padding='SAME')\n",
        "        dims = [init_dim, * map(lambda m: dim * m, dim_mults)]\n",
        "        in_out = list(zip(dims[:-1], dims[1:]))\n",
        "        block_klass = partial(ResnetBlock, groups = resnet_block_groups)\n",
        "        time_dim = dim * 4\n",
        "        self.sinusoidal_cond_mlp = sinusoidal_cond_mlp\n",
        "        self.time_mlp = Sequential([\n",
        "            SinusoidalPosEmb(dim),\n",
        "            nn.Dense(units=time_dim),\n",
        "            GELU(),\n",
        "            nn.Dense(units=time_dim)\n",
        "        ], name='time embeddings')\n",
        "        self.downs = []\n",
        "        self.ups = []\n",
        "        num_resolutions = len(in_out)\n",
        "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
        "            is_last = ind >= (num_resolutions - 1)\n",
        "            self.downs.append([\n",
        "                block_klass(dim_in, dim_out, time_emb_dim=time_dim),\n",
        "                block_klass(dim_out, dim_out, time_emb_dim=time_dim),\n",
        "                Residual(PreNorm(dim_out, LinearAttention(dim_out))),\n",
        "                Downsample(dim_out) if not is_last else Identity()])\n",
        "        mid_dim = dims[-1]\n",
        "        self.mid_block1 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
        "        self.mid_attn = Residual(PreNorm(mid_dim, Attention(mid_dim)))\n",
        "        self.mid_block2 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
        "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out[1:])):\n",
        "            is_last = ind >= (num_resolutions - 1)\n",
        "            self.ups.append([\n",
        "                block_klass(dim_out * 2, dim_in, time_emb_dim=time_dim),\n",
        "                block_klass(dim_in, dim_in, time_emb_dim=time_dim),\n",
        "                Residual(PreNorm(dim_in, LinearAttention(dim_in))),\n",
        "                Upsample(dim_in) if not is_last else Identity()])\n",
        "        default_out_dim = channels * (1 if not learned_variance else 2)\n",
        "        self.out_dim = default(out_dim, default_out_dim)\n",
        "        self.final_conv = Sequential([\n",
        "            block_klass(dim * 2, dim),\n",
        "            nn.Conv2D(filters=self.out_dim, kernel_size=1, strides=1)\n",
        "        ], name='output')\n",
        "    def call(self, x, time=None, training=True, **kwargs):\n",
        "        x = self.init_conv(x)\n",
        "        t = self.time_mlp(time)\n",
        "        h = []\n",
        "        for block1, block2, attn, downsample in self.downs:\n",
        "            x = block1(x, t)\n",
        "            x = block2(x, t)\n",
        "            x = attn(x)\n",
        "            h.append(x)\n",
        "            x = downsample(x)\n",
        "        x = self.mid_block1(x, t)\n",
        "        x = self.mid_attn(x)\n",
        "        x = self.mid_block2(x, t)\n",
        "        for block1, block2, attn, upsample in self.ups:\n",
        "            x = tf.concat([x, h.pop()], axis=-1)\n",
        "            x = block1(x, t)\n",
        "            x = block2(x, t)\n",
        "            x = attn(x)\n",
        "            x = upsample(x)\n",
        "        x = tf.concat([x, h.pop()], axis=-1)\n",
        "        x = self.final_conv(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "-HiIx21W5li2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train / Generate"
      ],
      "metadata": {
        "id": "5n5HVBcCYbGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epochs=1):\n",
        "    unet = Unet(channels=channels)\n",
        "    opt = keras.optimizers.Adam(learning_rate=1e-4)\n",
        "    for e in range(1, epochs + 1):\n",
        "        bar = tf.keras.utils.Progbar(len(dataset)-1)\n",
        "        losses = []\n",
        "        for i, batch in enumerate(iter(dataset)):\n",
        "            rng, tsrng = np.random.randint(0, 1e5, size=(2,))\n",
        "            timestep_values = generate_timestamp(tsrng, batch.shape[0])\n",
        "            noised_data, noise = forward_noise(rng, batch, timestep_values)\n",
        "            with tf.GradientTape() as tape:\n",
        "                prediction = unet(noised_data, timestep_values)\n",
        "                loss = loss_fn(noise, prediction)\n",
        "            gradients = tape.gradient(loss, unet.trainable_variables)\n",
        "            opt.apply_gradients(zip(gradients, unet.trainable_variables))\n",
        "            losses.append(loss)\n",
        "            bar.update(i, values=[('loss', loss)])\n",
        "        avg = np.mean(losses)\n",
        "        print(f'Average loss for epoch {e}/{epochs}: {avg}')\n",
        "        tff.learning.models.save(unet, './weights') ########## not saving/loading properly\n",
        "\n",
        "train(epochs)"
      ],
      "metadata": {
        "id": "hVBn5wBesReO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(inference_timesteps=10):\n",
        "    ''' Generate data '''\n",
        "    unet = tff.learning.models.load('./weights') ##########\n",
        "    inference_range = range(0, timesteps, timesteps // inference_timesteps)\n",
        "    x = np.random.normal(size=(1, shape[0], shape[1], channels))\n",
        "    for i in reversed(range(inference_timesteps)):\n",
        "        t = np.expand_dims(inference_range[i], 0)\n",
        "        pred_noise = unet(x, t)\n",
        "        x = ddim(x, pred_noise, t, 0)\n",
        "        output = np.squeeze(x, 0)\n",
        "        if channels == 1:\n",
        "            output = np.squeeze(output,-1)\n",
        "            plt.imshow(output, cmap='gray')\n",
        "            plt.show()\n",
        "        elif channels == 3:\n",
        "            plt.imshow(output, interpolation='nearest')\n",
        "            plt.show()\n",
        "\n",
        "inference()"
      ],
      "metadata": {
        "id": "cvCepSIoKd--"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}